{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be30038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272bc603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: /opt/anaconda3/envs/rag1/bin/python\n",
      "\n",
      "Python version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:45:25) [Clang 18.1.8 ]\n",
      "\n",
      "LibrerÃ­as instaladas:\n",
      "langchain                 0.3.27\n",
      "langchain-classic         1.0.0\n",
      "langchain-community       0.3.31\n",
      "langchain-core            1.0.4\n",
      "langchain-openai          0.3.35\n",
      "langchain-pinecone        0.2.13\n",
      "langchain-text-splitters  0.3.11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python path:\", sys.executable)\n",
    "print(\"\\nPython version:\", sys.version)\n",
    "\n",
    "# Ver quÃ© tienes instalado\n",
    "import subprocess\n",
    "result = subprocess.run([sys.executable, '-m', 'pip', 'list'], capture_output=True, text=True)\n",
    "print(\"\\nLibrerÃ­as instaladas:\")\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if 'langchain' in line.lower():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38452fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Configurar credenciales\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# 2. Conectar a Pinecone (aquÃ­ se conecta a la base de datos vectorial)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"document-index\",  # tu Ã­ndice en Pinecone\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 3. Crear el retriever (esto es lo que consulta Pinecone)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1254f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # o \"gpt-4o\" si quieres el mÃ¡s potente\n",
    "    temperature=0  # 0 para respuestas mÃ¡s determinÃ­sticas\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Crear el template del prompt\n",
    "system_prompt = (\n",
    "    \"Eres un asistente experto que responde preguntas basÃ¡ndote \"\n",
    "    \"Ãºnicamente en el contexto proporcionado. \"\n",
    "    \"Si no encuentras la respuesta en el contexto, di que no tienes \"\n",
    "    \"suficiente informaciÃ³n.\\n\\n\"\n",
    "    \"Contexto: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Esta cadena toma los documentos recuperados y los procesa con el LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Esta cadena conecta el retriever con el LLM\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92427f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Respuesta: Para crear productos a travÃ©s de una matriz, debes seguir estos pasos:\n",
      "\n",
      "1. Recopila la informaciÃ³n de los productos y descarga la tabla de equivalencias desde Productos -> Carga masiva -> Descargar tabla de equivalencias.\n",
      "2. Completa la matriz de carga, asegurÃ¡ndote de que no haya celdas vacÃ­as, fÃ³rmulas o errores. La matriz debe contener columnas con los distintos campos/propiedades de los productos y el cuerpo debe incluir los valores correspondientes a cada SKU.\n",
      "3. AsegÃºrate de trabajar en la pestaÃ±a \"Productos\", que debe estar ubicada en la primera posiciÃ³n a la izquierda. Las demÃ¡s pestaÃ±as pueden servir como complemento o fuente de datos.\n",
      "4. Una vez que la matriz estÃ© completa, guarda el archivo en formato Excel (XLSX).\n",
      "5. Ingresa a Productos -> Carga masiva -> \"Selecciona Archivo\" y selecciona el archivo que has creado.\n",
      "6. En 'Carga tu excel con productos', selecciona el archivo y haz clic en Subir y previsualizar.\n",
      "7. Luego de importar los productos, puedes proceder a cargar las imÃ¡genes si es necesario, desde \"Carga masiva\" â†’ \"ImportaciÃ³n de imÃ¡genes\".\n",
      "8. Finalmente, da por finalizada la importaciÃ³n.\n",
      "\n",
      "AsegÃºrate de seguir todos estos pasos para crear los productos correctamente.\n",
      "\n",
      "ðŸ“„ Fuentes utilizadas:\n",
      "\n",
      "--- Fuente 1 ---\n",
      "62\n",
      "Carga de matriz\n",
      "Una vez armado el archivo de la matriz, se debe guardar en formato Excel (XLSX).\n",
      "Para cargar el archivo:\n",
      "1.Ingresar a Productos -> Carga masiva -> \"Selecciona Archivo\":\n",
      "2.En 'Carga \n",
      "\n",
      "--- Fuente 2 ---\n",
      "56\n",
      "Armado de matriz\n",
      "Luego de recopilar la informaciÃ³n de los productos, se debe descargar la tabla de\n",
      "equivalencias desde Productos -> Carga masiva -> Descargar tabla de\n",
      "equivalencias\n",
      "La matriz de car\n",
      "\n",
      "--- Fuente 3 ---\n",
      "66\n",
      "6.Luego de importar los productos procedemos a cargar las imÃ¡genes, que se\n",
      "puede realizar seguidamente la carga de la matriz o en caso de cargar la matriz\n",
      "sin imÃ¡genes se puede realizar desde \"Carg\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Â¿cÃ³mo cargo imÃ¡genes a travÃ©s de dropebox?\"})\n",
    "\n",
    "# Ver la respuesta\n",
    "print(\"ðŸ¤– Respuesta:\", response[\"answer\"])\n",
    "print(\"\\nðŸ“„ Fuentes utilizadas:\")\n",
    "for i, doc in enumerate(response[\"context\"], 1):\n",
    "    print(f\"\\n--- Fuente {i} ---\")\n",
    "    print(doc.page_content[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
